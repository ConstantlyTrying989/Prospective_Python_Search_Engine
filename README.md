# Vector-Based Text Search Engine

A from-scratch implementation of a document search engine using TF-IDF vectorization and cosine similarity. Built entirely in Python with no pre-built search libraries, demonstrating core information retrieval concepts.

## ğŸ¯ Project Overview

This search engine indexes classic literature from Project Gutenberg and enables semantic search across multiple documents. The system preprocesses text, builds TF-IDF representations, and ranks documents using cosine similarity.

**Key Features:**
- Custom TF-IDF vectorizer implementation from scratch
- Configurable text preprocessing (stemming, stopword removal)
- Cosine similarity ranking for document retrieval
- Evaluation metrics (Precision@K, Recall@K, Average Precision)
- Interactive search interface
- Clean, modular architecture

## ğŸ› ï¸ Technologies Used

- **Python 3.9+**
- **NumPy** - Numerical computations and vector operations
- **Pandas** - Data manipulation
- **NLTK** - Tokenization and linguistic preprocessing
- **Scikit-learn** - Validation and comparison benchmarks
- **Matplotlib** - Evaluation visualizations

## ğŸ“ Project Structure
```
python-text-search-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader.py           # Document loading and management
â”‚   â”œâ”€â”€ preprocessing.py    # Text cleaning and tokenization
â”‚   â”œâ”€â”€ vectorizer.py       # TF-IDF implementation
â”‚   â”œâ”€â”€ search.py           # Search engine with cosine similarity
â”‚   â””â”€â”€ evaluation.py       # Performance metrics
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_texts/          # Project Gutenberg corpus (7 books)
â”œâ”€â”€ demo_search.py          # Interactive search interface
â”œâ”€â”€ test_*.py               # Unit tests for each module
â””â”€â”€ requirements.txt
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9 or higher
- Windows 10/11 (or macOS/Linux)
- Git

### Installation (Windows)

1. **Clone the repository:**
```cmd
   git clone https://github.com/YourUsername/python-text-search-engine.git
   cd python-text-search-engine
```

2. **Create virtual environment:**
```cmd
   python -m venv venv
   venv\Scripts\activate.bat
```

3. **Install dependencies:**
```cmd
   pip install -r requirements.txt
```

4. **Download NLTK data (first run only):**
```cmd
   python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Quick Start

**Run the interactive search demo:**
```cmd
python demo_search.py
```

**Example queries to try:**
- `detective mystery crime`
- `ocean whale adventure`
- `vampire blood night`
- `love romance marriage`

## ğŸ“Š How It Works

### 1. Document Loading
The system loads plain text documents from the Project Gutenberg corpus, currently indexing 7 classic novels (~7MB of text).

### 2. Text Preprocessing
- Converts text to lowercase
- Removes URLs, emails, and numbers
- Tokenizes using NLTK's word_tokenize
- Removes stopwords (common words like "the", "and")
- Applies Porter stemming to reduce words to root forms

### 3. TF-IDF Vectorization
Implements Term Frequency-Inverse Document Frequency from scratch:
- **TF (Term Frequency):** Normalized word frequency in document
- **IDF (Inverse Document Frequency):** log(N / document_frequency)
- Creates sparse vector representation for each document

### 4. Cosine Similarity Ranking
Compares query vector to document vectors using cosine similarity:
```
similarity = (A Â· B) / (||A|| Ã— ||B||)
```
Returns top K most similar documents ranked by score.

### 5. Evaluation Metrics
- **Precision@K:** Accuracy of top K results
- **Recall@K:** Coverage of relevant documents in top K
- **Average Precision:** Overall ranking quality

## ğŸ§ª Testing

**Run all tests:**
```cmd
python test_loader.py
python test_preprocessing.py
python test_vectorizer.py
python test_search.py
python test_evaluation.py
```

**Expected output:**
- Document loading verification
- Preprocessing statistics
- TF-IDF matrix dimensions
- Search result rankings
- Evaluation metric scores

## ğŸ“ˆ Performance

**Corpus Statistics:**
- Documents indexed: 7 classic novels
- Total tokens processed: ~316,000
- Vocabulary size: ~18,500 unique terms
- Average query time: <0.1 seconds

**Sample Results (Query: "detective mystery crime"):**
1. Adventures of Sherlock Holmes (Score: 0.4532)
2. Tale of Two Cities (Score: 0.1234)
3. Dracula (Score: 0.0876)

## ğŸ” Technical Highlights

**Custom Implementation:**
- No use of sklearn's TfidfVectorizer - built from scratch
- Manual cosine similarity computation
- Custom evaluation metrics

**Code Quality:**
- Type hints throughout
- Comprehensive docstrings
- Modular design with clear separation of concerns
- Unit tests for all components

**Algorithmic Complexity:**
- Indexing: O(N Ã— M) where N = documents, M = avg tokens
- Query: O(V) where V = vocabulary size
- Memory: O(N Ã— V) for document-term matrix

## ğŸ“ Learning Outcomes

This project demonstrates:
- Information retrieval fundamentals (TF-IDF, cosine similarity)
- Text preprocessing and NLP pipeline design
- NumPy for efficient vector operations
- Software engineering best practices (modularity, testing)
- Working with real-world text data

## ğŸ“ Future Enhancements

- [ ] Add BM25 ranking algorithm
- [ ] Implement query expansion with synonyms
- [ ] Add phrase search support
- [ ] Build web interface with Flask
- [ ] Optimize with sparse matrices for larger corpora
- [ ] Add relevance feedback mechanism

## ğŸ¤ Contributing

This is a portfolio project, but suggestions are welcome! Feel free to open an issue or submit a pull request.

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@ConstantlyTrying989](https://github.com/ConstantlyTrying989)
- LinkedIn: [Jason Lewis](https://linkedin.com/in/jason-lewis-b3a188288)
- Email: lewisjd2007@gmail.com

## ğŸ™ Acknowledgments

- Project Gutenberg for public domain texts
- NLTK team for NLP tools
- Classic literature authors for the corpus

---

**Built as part of internship application portfolio | 2026**